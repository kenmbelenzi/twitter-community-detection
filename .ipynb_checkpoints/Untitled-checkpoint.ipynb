{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from tweepy.streaming import StreamListener\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import re\n",
    "import fire\n",
    "#Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_tag_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_tag_elements('http://www.travelstart.co.ke/blog/the-50-best-kenyans-to-follow-on-twitter','ol')\n",
    "df = pd.DataFrame(res)\n",
    "\n",
    "df=df[0].str.split('@',expand = True)\n",
    "users=df[[0,1]]\n",
    "users.dropna()\n",
    "users = users.head(50)\n",
    "#users.to_csv('users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'OwaWIDBtZ6QOIFjcTH5i6tKPZ'\n",
    "consumer_secret = 'zsSqmcWmysfz42VgTlw401KmLNAYI2tvEUTW2ISX5vHnyoRHx8'\n",
    "access_token = '275506568-ASxyT5sfrVXM17QIlGL5j2Ws2Qv2PHIyH4Fc8U0M'\n",
    "access_token_secret = 'TE8xLOJQFD8uZrqS8xq3akXKy1aOBkVC6uOQ6A52Cvnc4'\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "    def on_erroe(self,status_code):\n",
    "        if status_code == 420:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to stream...\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "search_terms = ['economy','social','cultural','health']\n",
    "def stream_tweets(search_term):\n",
    "    data = []# empty list to which tweet_details obj will be added\n",
    "    counter = 0 # counter to keep track of each iteration\n",
    "    for tweet in tweepy.Cursor(api.search, q='\\\"{}\\\" -filter:retweets'.format(search_term), count=100, lang='en', tweet_mode='extended').items():\n",
    "        tweet_details = {}\n",
    "        tweet_details['name'] = tweet.user.screen_name\n",
    "        tweet_details['tweet'] = tweet.full_text\n",
    "        tweet_details['retweets'] = tweet.retweet_count\n",
    "        tweet_details['location'] = tweet.user.location\n",
    "        tweet_details['created'] = tweet.created_at.strftime(\"%d-%b-%Y\")\n",
    "        tweet_details['followers'] = tweet.user.followers_count\n",
    "        tweet_details['is_user_verified'] = tweet.user.verified\n",
    "        data.append(tweet_details)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter == 1000:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    with open('data/{}.json'.format(search_term), 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print('done!')\n",
    "if __name__ == \"__main__\":\n",
    "    print('Starting to stream...')\n",
    "    for search_term in search_terms:\n",
    "        stream_tweets(search_term)\n",
    "    print('finished!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenya=pd.read_json('data/kenya.json',orient='records')\n",
    "economy = pd.read_json('data/economy.json',orient='records')\n",
    "health = pd.read_json('data/health.json',orient='records')\n",
    "social = pd.read_json('data/social.json',orient='records')\n",
    "cultural = pd.read_json('data/cultural.json',orient='records')\n",
    "frames = [kenya,economy,health,social,cultural]\n",
    "tweets = pd.concat(frames, ignore_index=True)\n",
    "#tweets['location']\n",
    "location='Kenya'\n",
    "kenyans=tweets[tweets['location'].str.contains('Kenya|kenya')]\n",
    "#kenyans.to_csv('kenyans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco=health['tweet']\n",
    "\n",
    "pd.Series(' '.join(eco).lower().split()).value_counts()\n",
    "def wordcloud(eco):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in eco]))\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"most comon hashtags\")\n",
    "wordcloud(eco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "williamsRuto\n",
      "RailaOdinga\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('data/userlist.csv')\n",
    "users = users['Influencer'].replace('@','',regex=True)\n",
    "users\n",
    "username=[]\n",
    "for user in users:\n",
    "    print(user)\n",
    "    for user in tweepy.Cursor(api.friends, screen_name=user).items():\n",
    "        ken=pd.DataFrame()\n",
    "        friend = user.screen_name\n",
    "        username.append(friend)\n",
    "        ken=ken.append(username,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ken.to_csv('ken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
